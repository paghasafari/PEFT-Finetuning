{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for PEFT and evaluation\n",
    "import torch, evaluate, os\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "postfix=random.randint(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf3a6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if CUDA is available, otherwise fallback to CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad022f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a subset of the \"emotion\" dataset from the Hugging Face datasets library.\n",
    "emo_ds = load_dataset(\"emotion\", split=\"train[0:5000]\")\n",
    "\n",
    "# Splitting the loaded dataset into training and testing sets.\n",
    "emo_split_ds = emo_ds.train_test_split(test_size=0.2)\n",
    "splits = [\"train\", \"test\"]\n",
    "emo_split_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb288f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i am feeling suspicious lj cut text suspicions', 'label': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_split_ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8e4231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels' names in the training split of the dataset.\n",
    "emo_split_ds['train'].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b10abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {1 : \"sadness\", 2 : \"joy\", 3 : \"love\", 4 : \"anger\", 5 : \"fear\", 6: \"surprise\"}\n",
    "label2id = {\"sadness\" : 1, \"joy\" : 2, \"love\" : 3, \"anger\" : 4, \"fear\" : 5, \"surprise\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d94c7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\parya.aghasafari\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Linear8bitLt(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "# Configuring the BitsAndBytesConfig for 8-bit quantization.\n",
    "# - 'load_in_8bit=True' enables loading the model in 8-bit precision to reduce memory usage.\n",
    "# - 'bnb_8bit_quant_type=\"nf8\"' specifies the type of 8-bit quantization to use, in this case, \"nf8\" (Normalized Float 8-bit).\n",
    "# - 'bnb_8bit_compute_dtype=torch.float16' sets the computation to use 16-bit floating point precision during model operations.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_quant_type=\"nf8\",\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Loading a pre-trained GPT-2 model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=6, \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24ee306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00<00:00, 16955.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 15776.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i am feeling suspicious lj cut text suspicions', 'label': 4, 'input_ids': [72, 716, 4203, 13678, 300, 73, 2005, 2420, 30508, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "[72, 716, 4203, 13678, 300, 73, 2005, 2420, 30508, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the tokenizer associated with the pre-trained model \"gpt2\".\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Setting the padding token to End of Sentence token\n",
    "\n",
    "# Defining a preprocessing function to tokenize the input text.\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,  # Truncate the input text to the maximum length specified.\n",
    "        padding=\"max_length\",  # Pad all sequences to the maximum length.\n",
    "        max_length=128,  # Set the maximum length of the tokenized sequences to 128 tokens.\n",
    "        return_attention_mask=True, # Include attention masks in the output, which indicates padded tokens.\n",
    "        return_tensors=\"pt\",  # Ensure tensors are returned\n",
    "    )\n",
    "\n",
    "# Tokenizing each split (\"train\" and \"test\") of the dataset using the preprocessing function.\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = emo_split_ds[split].map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_ds[\"train\"][0])\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230f9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Calculate accuracy by comparing model predictions with true labels.\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6544570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.20k/4.20k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Processed 1/1000 examples...\n",
      "Processed 501/1000 examples...\n",
      "Pre-fine-tuning Evaluation Results: {'accuracy': 0.033}\n"
     ]
    }
   ],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id # Align the model's padding token ID with the token ID used by the tokenizer.\n",
    "\n",
    "# Freeze the base model parameters to prevent them from being updated during fine-tuning.\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Evaluation without Trainer\n",
    "# Load the accuracy metric for evaluating the model's performance.\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "eval_dataset = tokenized_ds[\"test\"]\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "for idx, example in enumerate(eval_dataset):\n",
    "    with torch.no_grad():\n",
    "        inputs = preprocess_function(example)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        all_labels.append(example[\"label\"])  # Append the single label\n",
    "        all_preds.extend(preds)  # Extend the predictions list\n",
    "\n",
    "    # Print progress every 500 examples\n",
    "    if idx % 500 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(eval_dataset)} examples...\")\n",
    "\n",
    "# Calculate the accuracy of the model by comparing all predictions with the true labels.\n",
    "accuracy = metric.compute(predictions=all_preds, references=all_labels)\n",
    "print(\"Pre-fine-tuning Evaluation Results:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27df3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA_LAUNCH_BLOCKING to catch CUDA errors more precisely\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Load the model without quantization and move it to the appropriate device (GPU if available)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=6,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(device)  # Move the model to the appropriate device (GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e6a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\parya.aghasafari\\AppData\\Local\\Temp\\ipykernel_39704\\860449212.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  pre_trainer = Trainer(\n",
      "                                                 \n",
      " 20%|â–ˆâ–ˆ        | 125/625 [00:47<02:42,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.799188613891602, 'eval_accuracy': 0.304, 'eval_runtime': 11.0202, 'eval_samples_per_second': 90.743, 'eval_steps_per_second': 2.904, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/625 [01:36<01:48,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.490077972412109, 'eval_accuracy': 0.304, 'eval_runtime': 8.8218, 'eval_samples_per_second': 113.355, 'eval_steps_per_second': 3.627, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 375/625 [02:22<01:12,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.592576026916504, 'eval_accuracy': 0.304, 'eval_runtime': 8.8196, 'eval_samples_per_second': 113.384, 'eval_steps_per_second': 3.628, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/625 [03:00<00:35,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9522, 'grad_norm': 155.72743225097656, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/625 [03:09<00:35,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.078474521636963, 'eval_accuracy': 0.304, 'eval_runtime': 8.7644, 'eval_samples_per_second': 114.098, 'eval_steps_per_second': 3.651, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [03:56<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.912605285644531, 'eval_accuracy': 0.304, 'eval_runtime': 8.7313, 'eval_samples_per_second': 114.531, 'eval_steps_per_second': 3.665, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [03:58<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 238.1659, 'train_samples_per_second': 83.975, 'train_steps_per_second': 2.624, 'train_loss': 6.59064140625, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.912605285644531,\n",
       " 'eval_accuracy': 0.304,\n",
       " 'eval_runtime': 8.7146,\n",
       " 'eval_samples_per_second': 114.75,\n",
       " 'eval_steps_per_second': 3.672,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Define training arguments\n",
    "training_args =TrainingArguments(\n",
    "        output_dir=model_name + \"-emo-pre-tuning-\" + str(postfix),\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "# Initialize the Trainer \n",
    "pre_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "pre_trainer.train()\n",
    "pre_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a39f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i got the feeling watching it that only from s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel so regretful not going but</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i know you cant just ged rid of your feelings ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel awful about not working this summer im ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess i m a sucker for the grand and endless...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>im not feeling joyful or spiritually fit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i feel this strategy is worthwhile</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i have struggled to fit all the work in for th...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i am not normally the kind of person who gets ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i started to feel dissatisfied by the ease and...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label\n",
       "0  i got the feeling watching it that only from s...      1                0\n",
       "1                  i feel so regretful not going but      0                0\n",
       "2  i know you cant just ged rid of your feelings ...      1                0\n",
       "3  i feel awful about not working this summer im ...      0                0\n",
       "4  i guess i m a sucker for the grand and endless...      1                0\n",
       "5           im not feeling joyful or spiritually fit      1                0\n",
       "6                 i feel this strategy is worthwhile      1                0\n",
       "7  i have struggled to fit all the work in for th...      3                0\n",
       "8  i am not normally the kind of person who gets ...      2                0\n",
       "9  i started to feel dissatisfied by the ease and...      3                0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "predictions = pre_trainer.predict(tokenized_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0065596",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa479c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\parya.aghasafari\\AppData\\Local\\Temp\\ipykernel_39704\\296279292.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n",
      "  0%|          | 0/2500 [00:00<?, ?it/s]c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 10%|â–ˆ         | 250/2500 [00:59<08:35,  4.37it/s]\n",
      " 10%|â–ˆ         | 250/2500 [01:06<08:35,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5746538639068604, 'eval_accuracy': 0.817, 'eval_runtime': 7.355, 'eval_samples_per_second': 135.962, 'eval_steps_per_second': 8.566, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 20%|â–ˆâ–ˆ        | 500/2500 [02:04<07:30,  4.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7337, 'grad_norm': 5.961215972900391, 'learning_rate': 0.0008032, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|â–ˆâ–ˆ        | 500/2500 [02:10<07:30,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3088444769382477, 'eval_accuracy': 0.896, 'eval_runtime': 5.5722, 'eval_samples_per_second': 179.461, 'eval_steps_per_second': 11.306, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [03:08<06:40,  4.37it/s]  \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [03:14<06:40,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2879340648651123, 'eval_accuracy': 0.906, 'eval_runtime': 5.6369, 'eval_samples_per_second': 177.404, 'eval_steps_per_second': 11.176, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [04:12<05:42,  4.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.247, 'grad_norm': 3.7211077213287354, 'learning_rate': 0.0006031999999999999, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [04:18<05:42,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29920694231987, 'eval_accuracy': 0.922, 'eval_runtime': 5.6407, 'eval_samples_per_second': 177.284, 'eval_steps_per_second': 11.169, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [05:17<04:51,  4.29it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [05:22<04:51,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25252997875213623, 'eval_accuracy': 0.935, 'eval_runtime': 5.549, 'eval_samples_per_second': 180.213, 'eval_steps_per_second': 11.353, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [06:25<03:49,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1545, 'grad_norm': 9.811891555786133, 'learning_rate': 0.0004032, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [06:31<03:49,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24325823783874512, 'eval_accuracy': 0.931, 'eval_runtime': 5.4531, 'eval_samples_per_second': 183.382, 'eval_steps_per_second': 11.553, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                   \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1750/2500 [07:34<02:51,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27847930788993835, 'eval_accuracy': 0.926, 'eval_runtime': 5.4354, 'eval_samples_per_second': 183.978, 'eval_steps_per_second': 11.591, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [08:31<01:51,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0971, 'grad_norm': 1.6098593473434448, 'learning_rate': 0.0002032, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [08:36<01:51,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2658595144748688, 'eval_accuracy': 0.928, 'eval_runtime': 5.4764, 'eval_samples_per_second': 182.602, 'eval_steps_per_second': 11.504, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                   \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2250/2500 [09:40<00:57,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3127102255821228, 'eval_accuracy': 0.93, 'eval_runtime': 5.5567, 'eval_samples_per_second': 179.962, 'eval_steps_per_second': 11.338, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [10:38<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0599, 'grad_norm': 1.1508469581604004, 'learning_rate': 3.2000000000000003e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: af9ef720-dfdc-4bed-8fb4-598151fd2f4d)') - silently ignoring the lookup for the file config.json in gpt2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\parya.aghasafari\\Documents\\projects\\Udacity\\Part1\\projects\\PEFT-Finetuning\\peft\\Lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in gpt2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [10:54<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2961682677268982, 'eval_accuracy': 0.933, 'eval_runtime': 5.7425, 'eval_samples_per_second': 174.139, 'eval_steps_per_second': 10.971, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [10:57<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 657.36, 'train_samples_per_second': 60.849, 'train_steps_per_second': 3.803, 'train_loss': 0.25843915786743166, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:05<00:00, 11.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24325823783874512,\n",
       " 'eval_accuracy': 0.931,\n",
       " 'eval_runtime': 5.7356,\n",
       " 'eval_samples_per_second': 174.349,\n",
       " 'eval_steps_per_second': 10.984,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Applying LoRA to Conv1D layers in GPT2Attention\n",
    "    bias=\"none\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config).to(device)  # Move PEFT model to GPU\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name + \"-lora-peft-fine-tuning-\" + str(postfix),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Enable mixed precision for faster training on GPU\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for fine-tuning\n",
    "ft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# train model\n",
    "ft_trainer.train()\n",
    "ft_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d59d53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.save_pretrained(\"emo-gpt2-peft-fine-tuned\", save_adapter=True, save_config=True)\n",
    "peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76fa94f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:04<00:00, 14.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i got the feeling watching it that only from s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel so regretful not going but</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i know you cant just ged rid of your feelings ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel awful about not working this summer im ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess i m a sucker for the grand and endless...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>im not feeling joyful or spiritually fit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i feel this strategy is worthwhile</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i have struggled to fit all the work in for th...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i am not normally the kind of person who gets ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i started to feel dissatisfied by the ease and...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label\n",
       "0  i got the feeling watching it that only from s...      1                1\n",
       "1                  i feel so regretful not going but      0                0\n",
       "2  i know you cant just ged rid of your feelings ...      1                1\n",
       "3  i feel awful about not working this summer im ...      0                0\n",
       "4  i guess i m a sucker for the grand and endless...      1                1\n",
       "5           im not feeling joyful or spiritually fit      1                1\n",
       "6                 i feel this strategy is worthwhile      1                1\n",
       "7  i have struggled to fit all the work in for th...      3                3\n",
       "8  i am not normally the kind of person who gets ...      2                2\n",
       "9  i started to feel dissatisfied by the ease and...      3                3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "predictions = ft_trainer.predict(tokenized_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863ec66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved PEFT model\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"emo-gpt2-peft-fine-tuned\",  num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80321308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "infer_input = tokenizer(\n",
    "        'one of the best restaurants i ever visited', \n",
    "        truncation=True,  \n",
    "        padding=\"max_length\", \n",
    "        max_length=128,  \n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "with torch.no_grad():\n",
    "    infer_output = lora_model(**infer_input)\n",
    "    logits = infer_output.logits\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).numpy()[0]\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4deb8793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "infer_input = tokenizer(\n",
    "        'I hate this city', \n",
    "        truncation=True,  \n",
    "        padding=\"max_length\", \n",
    "        max_length=128,  \n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "with torch.no_grad():\n",
    "    infer_output = lora_model(**infer_input)\n",
    "    logits = infer_output.logits\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).numpy()[0]\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2687eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
